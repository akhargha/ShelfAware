{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1s_WUrYBtlMshII2Excn8L1NwwmtcxwE6",
      "authorship_tag": "ABX9TyP6tXz03XOOJyQ3yPIcY5cD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akhargha/ShelfAware/blob/main/ML/fine_tuning_tutorial_MobileNetV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwDua1wsa5DD",
        "outputId": "22c55432-1354-43ee-be7b-bd4342f2ce15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5 images belonging to 3 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-917c7d72fd92>:33: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  base_model = MobileNetV2(weights='imagenet', include_top=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9s/step - accuracy: 0.0000e+00 - loss: 2.0470 - val_accuracy: 0.4000 - val_loss: 0.9447\n",
            "Epoch 2/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 0.0000e+00 - loss: 1.4070 - val_accuracy: 0.2000 - val_loss: 0.9391\n",
            "Epoch 3/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.4000 - loss: 1.0678 - val_accuracy: 0.4000 - val_loss: 1.0800\n",
            "Epoch 4/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.4000 - loss: 0.9204 - val_accuracy: 0.4000 - val_loss: 1.0737\n",
            "Epoch 5/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.4000 - loss: 0.7750 - val_accuracy: 0.8000 - val_loss: 0.6334\n",
            "Epoch 6/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.5795 - val_accuracy: 1.0000 - val_loss: 0.6160\n",
            "Epoch 7/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.6000 - loss: 0.7183 - val_accuracy: 0.8000 - val_loss: 0.7052\n",
            "Epoch 8/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.4052 - val_accuracy: 1.0000 - val_loss: 0.4419\n",
            "Epoch 9/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.5943 - val_accuracy: 1.0000 - val_loss: 0.3265\n",
            "Epoch 10/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.4219 - val_accuracy: 1.0000 - val_loss: 0.3792\n",
            "Epoch 11/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.3699 - val_accuracy: 1.0000 - val_loss: 0.2270\n",
            "Epoch 12/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.1602 - val_accuracy: 1.0000 - val_loss: 0.3048\n",
            "Epoch 13/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.3134 - val_accuracy: 1.0000 - val_loss: 0.1888\n",
            "Epoch 14/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 1.0000 - loss: 0.3278 - val_accuracy: 1.0000 - val_loss: 0.1994\n",
            "Epoch 15/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.1550 - val_accuracy: 1.0000 - val_loss: 0.1288\n",
            "Epoch 16/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.2115 - val_accuracy: 1.0000 - val_loss: 0.1467\n",
            "Epoch 17/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.2267 - val_accuracy: 1.0000 - val_loss: 0.0789\n",
            "Epoch 18/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.1284 - val_accuracy: 1.0000 - val_loss: 0.0782\n",
            "Epoch 19/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.1375 - val_accuracy: 1.0000 - val_loss: 0.1205\n",
            "Epoch 20/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0973 - val_accuracy: 1.0000 - val_loss: 0.0618\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Predicted class: juice\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, image\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "\n",
        "# 1. Data Preparation\n",
        "data_dir = '/content/data/few_shot/'\n",
        "\n",
        "# Configure data augmentation for training\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,        # Randomly rotate images by up to 20 degrees\n",
        "    width_shift_range=0.2,    # Randomly shift images horizontally\n",
        "    height_shift_range=0.2,   # Randomly shift images vertically\n",
        "    shear_range=0.15,         # Randomly apply shear transformations\n",
        "    zoom_range=0.2,           # Randomly zoom in on images\n",
        "    horizontal_flip=True,     # Randomly flip images horizontally\n",
        "    rescale=1.0/255           # Normalize pixel values to the range [0, 1]\n",
        ")\n",
        "\n",
        "# Load training data\n",
        "train_data = datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(224, 224),   # Resize images to 224x224\n",
        "    batch_size=16,            # Use a batch size of 16\n",
        "    class_mode='categorical'  # Use categorical labels\n",
        ")\n",
        "\n",
        "# Check class labels\n",
        "class_labels = list(train_data.class_indices.keys())\n",
        "print(f\"Class Labels: {class_labels}\")\n",
        "\n",
        "# 2. Model Construction\n",
        "# Load the base MobileNetV2 model with ImageNet weights, excluding the top layer\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False)\n",
        "base_model.trainable = False  # Freeze the base model layers\n",
        "\n",
        "# Add custom layers on top of the base model\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)  # Add a global average pooling layer\n",
        "x = Dropout(0.2)(x)              # Add dropout for regularization\n",
        "x = Dense(128, activation='relu')(x)  # Add a fully connected layer\n",
        "output = Dense(train_data.num_classes, activation='softmax')(x)  # Output layer for classification\n",
        "\n",
        "# Define the final model\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# 3. Model Compilation and Training\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.0001),  # Use Adam optimizer with a low learning rate\n",
        "    loss='categorical_crossentropy',      # Use categorical crossentropy as the loss function\n",
        "    metrics=['accuracy']                  # Track accuracy during training\n",
        ")\n",
        "\n",
        "# Configure early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    train_data,\n",
        "    epochs=50,  # Train for a maximum of 50 epochs\n",
        "    steps_per_epoch=train_data.samples // train_data.batch_size,  # Number of steps per epoch\n",
        "    validation_data=train_data,  # Use the training data for validation\n",
        "    validation_steps=train_data.samples // train_data.batch_size,  # Number of validation steps\n",
        "    callbacks=[early_stopping]  # Use early stopping during training\n",
        ")\n",
        "\n",
        "# 4. Save the Model\n",
        "model.save('/content/fine_tuned_model.h5')  # Save the fine-tuned model\n",
        "\n",
        "# 5. Test Prediction\n",
        "# Load the saved fine-tuned model\n",
        "fine_tuned_model = tf.keras.models.load_model('/content/fine_tuned_model.h5')\n",
        "\n",
        "# Load a test image and preprocess it\n",
        "test_image_path = '/content/data/test_image.jpg'\n",
        "img = image.load_img(test_image_path, target_size=(224, 224))  # Resize the image to 224x224\n",
        "img_array = np.expand_dims(image.img_to_array(img), axis=0) / 255.0  # Normalize pixel values and expand dimensions\n",
        "\n",
        "# Make a prediction using the fine-tuned model\n",
        "predictions = fine_tuned_model.predict(img_array)\n",
        "predicted_class_index = np.argmax(predictions)  # Get the index of the class with the highest probability\n",
        "predicted_class = class_labels[predicted_class_index]  # Map the index to the corresponding class label\n",
        "\n",
        "# Output the predicted class\n",
        "print(f\"Predicted Class: {predicted_class}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications import imagenet_utils\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# 1. Load Fine-Tuned Model\n",
        "fine_tuned_model_path = '/Users/lucashyun/Trinity/weshack/model/fine_tuned_model.h5'  # Path to the Fine-Tuned model\n",
        "fine_tuned_model = tf.keras.models.load_model(fine_tuned_model_path)\n",
        "\n",
        "# Labels for the Fine-Tuned model\n",
        "fine_tuned_labels = ['beverage', 'juice']  # Class names used during Fine-Tuning\n",
        "\n",
        "# 2. Initialize MobileNetV2 (ImageNet Pretrained Model)\n",
        "imagenet_model = tf.keras.applications.MobileNetV2(weights='imagenet')\n",
        "\n",
        "# 3. Load and Display Image\n",
        "filename = '/Users/lucashyun/Trinity/weshack/detected_objects/cluster_0.png'  # Path to the test image\n",
        "img = image.load_img(filename, target_size=(224, 224))  # Resize the image to 224x224\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# 4. Preprocess the Image\n",
        "resizedimg = image.img_to_array(img)  # Convert the image to a NumPy array\n",
        "finalimg = np.expand_dims(resizedimg, axis=0)  # Expand dimensions to match input shape\n",
        "finalimg = tf.keras.applications.mobilenet_v2.preprocess_input(finalimg)  # Preprocess for ImageNet model\n",
        "\n",
        "# 5. Make Predictions with MobileNetV2 (ImageNet)\n",
        "imagenet_predictions = imagenet_model.predict(finalimg)\n",
        "imagenet_results = imagenet_utils.decode_predictions(imagenet_predictions, top=3)\n",
        "\n",
        "# 6. Make Predictions with Fine-Tuned Model\n",
        "fine_tuned_predictions = fine_tuned_model.predict(finalimg / 255.0)  # Fine-Tuned model expects [0, 1] normalized data\n",
        "fine_tuned_index = np.argmax(fine_tuned_predictions)\n",
        "fine_tuned_label = fine_tuned_labels[fine_tuned_index]\n",
        "fine_tuned_confidence = fine_tuned_predictions[0][fine_tuned_index]\n",
        "\n",
        "# 7. Output Results\n",
        "print(f\"Image File: {filename}\")\n",
        "\n",
        "# Print predictions from ImageNet model\n",
        "print(\"\\n[ImageNet (Pretrained MobileNetV2) Predictions]\")\n",
        "for i, (imagenet_id, label, confidence) in enumerate(imagenet_results[0]):\n",
        "    print(f\"{i + 1}. {label} (Confidence: {confidence:.2f})\")\n",
        "\n",
        "# Print predictions from Fine-Tuned model\n",
        "print(\"\\n[Fine-Tuned Model Prediction]\")\n",
        "print(f\"Predicted Label: {fine_tuned_label} (Confidence: {fine_tuned_confidence:.2f})\")\n",
        "\n",
        "# Final decision: Compare Fine-Tuned model and ImageNet model predictions\n",
        "print(\"\\n[Final Decision]\")\n",
        "if fine_tuned_confidence > 0.5:  # Prioritize Fine-Tuned model if confidence is above 0.5\n",
        "    print(f\"Result: {fine_tuned_label} (from Fine-Tuned Model)\")\n",
        "else:\n",
        "    print(f\"Result: {imagenet_results[0][0][1]} (from ImageNet Model)\")\n"
      ],
      "metadata": {
        "id": "8WWmW7t7ILQX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}